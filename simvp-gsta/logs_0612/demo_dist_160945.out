/ext3/miniconda3/bin:/ext3/miniconda3/bin:/ext3/miniconda3/condabin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/scratch/rss9311/pytorch-env-dir/simvpOpenStl
loading config from /scratch/rss9311/pytorch-env-dir/simvpOpenStl/configs/movingphy/simvp/SimVP_gSTA.py ...
loading config from /scratch/rss9311/pytorch-env-dir/simvpOpenStl/configs/movingphy/simvp/SimVP_gSTA.py ...
loading config from /scratch/rss9311/pytorch-env-dir/simvpOpenStl/configs/movingphy/simvp/SimVP_gSTA.py ...
loading config from /scratch/rss9311/pytorch-env-dir/simvpOpenStl/configs/movingphy/simvp/SimVP_gSTA.py ...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> training <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> training <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> training <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> training <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Use distributed mode with GPUs: local rank=2
Use distributed mode with GPUs: local rank=0
Use distributed mode with GPUs: local rank=3
Use distributed mode with GPUs: local rank=1
Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0]
CUDA available: True
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.7.r11.7/compiler.31442593_0
GPU 0,1,2,3: Tesla V100-SXM2-16GB
GCC: gcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
PyTorch: 2.0.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.16.1+cu121
OpenCV: 4.7.0
openstl: 0.3.0
------------------------------------------------------------

AMP not enabled. Training in float32.
AMP not enabled. Training in float32.
AMP not enabled. Training in float32.
AMP not enabled. Training in float32.

device: 	cuda	
dist: 	True	
display_step: 	10	
res_dir: 	work_dirs	
ex_name: 	exp_dist4.0_0612_5_30PM	
use_gpu: 	True	
fp16: 	False	
torchscript: 	False	
seed: 	42	
diff_seed: 	False	
fps: 	False	
empty_cache: 	True	
find_unused_parameters: 	False	
broadcast_buffers: 	True	
resume_from: 	checkpoint.pth	
auto_resume: 	False	
test: 	False	
inference: 	False	
deterministic: 	False	
launcher: 	pytorch	
local_rank: 	0	
port: 	29500	
batch_size: 	4	
val_batch_size: 	4	
num_workers: 	4	
data_root: 	/scratch/rss9311/squashfs-root/dataset	
dataname: 	movingphy	
pre_seq_length: 	11	
aft_seq_length: 	11	
total_length: 	22	
use_augment: 	False	
use_prefetcher: 	False	
drop_last: 	False	
method: 	simvp	
config_file: 	/scratch/rss9311/pytorch-env-dir/simvpOpenStl/configs/movingphy/simvp/SimVP_gSTA.py	
model_type: 	gSTA	
drop: 	0.0	
drop_path: 	0	
overwrite: 	False	
epoch: 	1	
log_step: 	1	
opt: 	adam	
opt_eps: 	None	
opt_betas: 	None	
momentum: 	0.9	
weight_decay: 	0.0	
clip_grad: 	None	
clip_mode: 	norm	
early_stop_epoch: 	-1	
no_display_method_info: 	False	
sched: 	onecycle	
lr: 	0.001	
lr_k_decay: 	1.0	
warmup_lr: 	1e-05	
min_lr: 	1e-06	
final_div_factor: 	10000.0	
warmup_epoch: 	0	
decay_epoch: 	100	
decay_rate: 	0.1	
filter_bias_and_bn: 	False	
spatio_kernel_enc: 	3	
spatio_kernel_dec: 	3	
hid_S: 	64	
hid_T: 	512	
N_T: 	8	
N_S: 	4	
in_shape: 	[11, 3, 160, 240]	
metrics: 	['mse', 'mae', 'ssim']	
Model info:
DistributedDataParallel(
  (module): SimVP_Model(
    (enc): Encoder(
      (enc): Sequential(
        (0): ConvSC(
          (conv): BasicConv2d(
            (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
        (1): ConvSC(
          (conv): BasicConv2d(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
        (2): ConvSC(
          (conv): BasicConv2d(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
        (3): ConvSC(
          (conv): BasicConv2d(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
      )
    )
    (dec): Decoder(
      (dec): Sequential(
        (0): ConvSC(
          (conv): BasicConv2d(
            (conv): Sequential(
              (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): PixelShuffle(upscale_factor=2)
            )
            (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
        (1): ConvSC(
          (conv): BasicConv2d(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
        (2): ConvSC(
          (conv): BasicConv2d(
            (conv): Sequential(
              (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): PixelShuffle(upscale_factor=2)
            )
            (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
        (3): ConvSC(
          (conv): BasicConv2d(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm): GroupNorm(2, 64, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
      )
      (readout): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
    )
    (hid): MidMetaNet(
      (enc): Sequential(
        (0): MetaBlock(
          (block): GASubBlock(
            (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): SpatialAttention(
              (proj_1): Conv2d(704, 704, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): AttentionModule(
                (conv0): Conv2d(704, 704, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=704)
                (conv_spatial): Conv2d(704, 704, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=704)
                (conv1): Conv2d(704, 1408, kernel_size=(1, 1), stride=(1, 1))
              )
              (proj_2): Conv2d(704, 704, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.010)
            (norm2): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(704, 5632, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(5632, 5632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5632)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(5632, 704, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (reduction): Conv2d(704, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): MetaBlock(
          (block): GASubBlock(
            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): SpatialAttention(
              (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): AttentionModule(
                (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
                (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
                (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              )
              (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): MetaBlock(
          (block): GASubBlock(
            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): SpatialAttention(
              (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): AttentionModule(
                (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
                (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
                (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              )
              (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.007)
            (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): MetaBlock(
          (block): GASubBlock(
            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): SpatialAttention(
              (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): AttentionModule(
                (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
                (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
                (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              )
              (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (4): MetaBlock(
          (block): GASubBlock(
            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): SpatialAttention(
              (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): AttentionModule(
                (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
                (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
                (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              )
              (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.004)
            (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (5): MetaBlock(
          (block): GASubBlock(
            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): SpatialAttention(
              (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): AttentionModule(
                (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
                (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
                (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              )
              (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (6): MetaBlock(
          (block): GASubBlock(
            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): SpatialAttention(
              (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): AttentionModule(
                (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
                (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
                (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              )
              (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.001)
            (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (7): MetaBlock(
          (block): GASubBlock(
            (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): SpatialAttention(
              (proj_1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): AttentionModule(
                (conv0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)
                (conv_spatial): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=512)
                (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
              )
              (proj_2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): Identity()
            (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4096)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(4096, 512, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (reduction): Conv2d(512, 704, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
)
| module                           | #parameters or shape   | #flops     |
|:---------------------------------|:-----------------------|:-----------|
| module                           | 48.568M                | 0.164T     |
|  enc.enc                         |  0.113M                |  9.7G      |
|   enc.enc.0.conv                 |   1.92K                |   0.865G   |
|    enc.enc.0.conv.conv           |    1.792K              |    0.73G   |
|    enc.enc.0.conv.norm           |    0.128K              |    0.135G  |
|   enc.enc.1.conv                 |   37.056K              |   3.927G   |
|    enc.enc.1.conv.conv           |    36.928K             |    3.893G  |
|    enc.enc.1.conv.norm           |    0.128K              |    33.792M |
|   enc.enc.2.conv                 |   37.056K              |   3.927G   |
|    enc.enc.2.conv.conv           |    36.928K             |    3.893G  |
|    enc.enc.2.conv.norm           |    0.128K              |    33.792M |
|   enc.enc.3.conv                 |   37.056K              |   0.982G   |
|    enc.enc.3.conv.conv           |    36.928K             |    0.973G  |
|    enc.enc.3.conv.norm           |    0.128K              |    8.448M  |
|  dec                             |  0.37M                 |  39.347G   |
|   dec.dec                        |   0.37M                |   39.266G  |
|    dec.dec.0.conv                |    0.148M              |    3.927G  |
|    dec.dec.1.conv                |    37.056K             |    3.927G  |
|    dec.dec.2.conv                |    0.148M              |    15.707G |
|    dec.dec.3.conv                |    37.056K             |    15.707G |
|   dec.readout                    |   0.195K               |   81.101M  |
|    dec.readout.weight            |    (3, 64, 1, 1)       |            |
|    dec.readout.bias              |    (3,)                |            |
|  hid.enc                         |  48.085M               |  0.115T    |
|   hid.enc.0                      |   10.396M              |   24.908G  |
|    hid.enc.0.block               |    10.036M             |    24.043G |
|    hid.enc.0.reduction           |    0.361M              |    0.865G  |
|   hid.enc.1.block                |   5.332M               |   12.767G  |
|    hid.enc.1.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.1.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.1.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.1.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.1.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.1.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.2.block                |   5.332M               |   12.767G  |
|    hid.enc.2.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.2.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.2.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.2.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.2.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.2.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.3.block                |   5.332M               |   12.767G  |
|    hid.enc.3.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.3.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.3.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.3.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.3.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.3.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.4.block                |   5.332M               |   12.767G  |
|    hid.enc.4.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.4.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.4.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.4.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.4.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.4.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.5.block                |   5.332M               |   12.767G  |
|    hid.enc.5.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.5.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.5.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.5.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.5.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.5.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.6.block                |   5.332M               |   12.767G  |
|    hid.enc.6.block.layer_scale_1 |    (512,)              |            |
|    hid.enc.6.block.layer_scale_2 |    (512,)              |            |
|    hid.enc.6.block.norm1         |    1.024K              |    2.458M  |
|    hid.enc.6.block.attn          |    1.09M               |    2.608G  |
|    hid.enc.6.block.norm2         |    1.024K              |    2.458M  |
|    hid.enc.6.block.mlp           |    4.24M               |    10.155G |
|   hid.enc.7                      |   5.694M               |   13.632G  |
|    hid.enc.7.block               |    5.332M              |    12.767G |
|    hid.enc.7.reduction           |    0.361M              |    0.865G  |
--------------------------------------------------------------------------------

[                                                  ] 0/88, elapsed: 0s, ETA:[                                  ] 1/88, 0.9 task/s, elapsed: 1s, ETA:    99s[                                  ] 2/88, 1.4 task/s, elapsed: 1s, ETA:    60s[>                                 ] 3/88, 1.9 task/s, elapsed: 2s, ETA:    45s[>                                 ] 4/88, 2.2 task/s, elapsed: 2s, ETA:    38s[>                                 ] 5/88, 2.5 task/s, elapsed: 2s, ETA:    34s[>>                                ] 6/88, 2.7 task/s, elapsed: 2s, ETA:    30s[>>                                ] 7/88, 2.9 task/s, elapsed: 2s, ETA:    28s[>>>                               ] 8/88, 3.0 task/s, elapsed: 3s, ETA:    26s[>>>                               ] 9/88, 3.2 task/s, elapsed: 3s, ETA:    25s[>>>                              ] 10/88, 3.3 task/s, elapsed: 3s, ETA:    24s[>>>>                             ] 11/88, 3.4 task/s, elapsed: 3s, ETA:    23s[>>>>                             ] 12/88, 3.5 task/s, elapsed: 3s, ETA:    22s[>>>>                             ] 13/88, 3.6 task/s, elapsed: 4s, ETA:    21s[>>>>>                            ] 14/88, 3.6 task/s, elapsed: 4s, ETA:    20s[>>>>>                            ] 15/88, 3.7 task/s, elapsed: 4s, ETA:    20s[>>>>>>                           ] 16/88, 3.7 task/s, elapsed: 4s, ETA:    19s[>>>>>>                           ] 17/88, 3.8 task/s, elapsed: 4s, ETA:    19s[>>>>>>                           ] 18/88, 3.8 task/s, elapsed: 5s, ETA:    18s[>>>>>>>                          ] 19/88, 3.9 task/s, elapsed: 5s, ETA:    18s[>>>>>>>                          ] 20/88, 3.9 task/s, elapsed: 5s, ETA:    17s[>>>>>>>                          ] 21/88, 3.9 task/s, elapsed: 5s, ETA:    17s[>>>>>>>>                         ] 22/88, 4.0 task/s, elapsed: 6s, ETA:    17s[>>>>>>>>                         ] 23/88, 4.0 task/s, elapsed: 6s, ETA:    16s[>>>>>>>>>                        ] 24/88, 4.0 task/s, elapsed: 6s, ETA:    16s[>>>>>>>>>                        ] 25/88, 4.1 task/s, elapsed: 6s, ETA:    15s[>>>>>>>>>                        ] 26/88, 4.1 task/s, elapsed: 6s, ETA:    15s[>>>>>>>>>>                       ] 27/88, 4.1 task/s, elapsed: 7s, ETA:    15s[>>>>>>>>>>                       ] 28/88, 4.1 task/s, elapsed: 7s, ETA:    14s[>>>>>>>>>>                       ] 29/88, 4.2 task/s, elapsed: 7s, ETA:    14s[>>>>>>>>>>>                      ] 30/88, 4.2 task/s, elapsed: 7s, ETA:    14s[>>>>>>>>>>>                      ] 31/88, 4.2 task/s, elapsed: 7s, ETA:    14s[>>>>>>>>>>>>                     ] 32/88, 4.2 task/s, elapsed: 8s, ETA:    13s[>>>>>>>>>>>>                     ] 33/88, 4.2 task/s, elapsed: 8s, ETA:    13s[>>>>>>>>>>>>                     ] 34/88, 4.2 task/s, elapsed: 8s, ETA:    13s[>>>>>>>>>>>>>                    ] 35/88, 4.2 task/s, elapsed: 8s, ETA:    12s[>>>>>>>>>>>>>                    ] 36/88, 4.3 task/s, elapsed: 8s, ETA:    12s[>>>>>>>>>>>>>                    ] 37/88, 4.3 task/s, elapsed: 9s, ETA:    12s[>>>>>>>>>>>>>>                   ] 38/88, 4.3 task/s, elapsed: 9s, ETA:    12s[>>>>>>>>>>>>>>                   ] 39/88, 4.3 task/s, elapsed: 9s, ETA:    11s[>>>>>>>>>>>>>>>                  ] 40/88, 4.3 task/s, elapsed: 9s, ETA:    11s[>>>>>>>>>>>>>>                  ] 41/88, 4.3 task/s, elapsed: 10s, ETA:    11s[>>>>>>>>>>>>>>>                 ] 42/88, 4.3 task/s, elapsed: 10s, ETA:    11s[>>>>>>>>>>>>>>>                 ] 43/88, 4.3 task/s, elapsed: 10s, ETA:    10s[>>>>>>>>>>>>>>>>                ] 44/88, 4.3 task/s, elapsed: 10s, ETA:    10s[>>>>>>>>>>>>>>>>                ] 45/88, 4.3 task/s, elapsed: 10s, ETA:    10s[>>>>>>>>>>>>>>>>                ] 46/88, 4.3 task/s, elapsed: 11s, ETA:    10s[>>>>>>>>>>>>>>>>>               ] 47/88, 4.4 task/s, elapsed: 11s, ETA:     9s[>>>>>>>>>>>>>>>>>               ] 48/88, 4.4 task/s, elapsed: 11s, ETA:     9s[>>>>>>>>>>>>>>>>>               ] 49/88, 4.4 task/s, elapsed: 11s, ETA:     9s[>>>>>>>>>>>>>>>>>>              ] 50/88, 4.4 task/s, elapsed: 11s, ETA:     9s[>>>>>>>>>>>>>>>>>>              ] 51/88, 4.4 task/s, elapsed: 12s, ETA:     8s[>>>>>>>>>>>>>>>>>>              ] 52/88, 4.4 task/s, elapsed: 12s, ETA:     8s[>>>>>>>>>>>>>>>>>>>             ] 53/88, 4.4 task/s, elapsed: 12s, ETA:     8s[>>>>>>>>>>>>>>>>>>>             ] 54/88, 4.4 task/s, elapsed: 12s, ETA:     8s[>>>>>>>>>>>>>>>>>>>>            ] 55/88, 4.4 task/s, elapsed: 12s, ETA:     7s[>>>>>>>>>>>>>>>>>>>>            ] 56/88, 4.4 task/s, elapsed: 13s, ETA:     7s[>>>>>>>>>>>>>>>>>>>>            ] 57/88, 4.4 task/s, elapsed: 13s, ETA:     7s[>>>>>>>>>>>>>>>>>>>>>           ] 58/88, 4.4 task/s, elapsed: 13s, ETA:     7s[>>>>>>>>>>>>>>>>>>>>>           ] 59/88, 4.4 task/s, elapsed: 13s, ETA:     7s[>>>>>>>>>>>>>>>>>>>>>           ] 60/88, 4.4 task/s, elapsed: 14s, ETA:     6s[>>>>>>>>>>>>>>>>>>>>>>          ] 61/88, 4.4 task/s, elapsed: 14s, ETA:     6s[>>>>>>>>>>>>>>>>>>>>>>          ] 62/88, 4.4 task/s, elapsed: 14s, ETA:     6s[>>>>>>>>>>>>>>>>>>>>>>          ] 63/88, 4.4 task/s, elapsed: 14s, ETA:     6s[>>>>>>>>>>>>>>>>>>>>>>>         ] 64/88, 4.4 task/s, elapsed: 14s, ETA:     5s[>>>>>>>>>>>>>>>>>>>>>>>         ] 65/88, 4.4 task/s, elapsed: 15s, ETA:     5s[>>>>>>>>>>>>>>>>>>>>>>>>        ] 66/88, 4.4 task/s, elapsed: 15s, ETA:     5s[>>>>>>>>>>>>>>>>>>>>>>>>        ] 67/88, 4.5 task/s, elapsed: 15s, ETA:     5s[>>>>>>>>>>>>>>>>>>>>>>>>        ] 68/88, 4.5 task/s, elapsed: 15s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>>>       ] 69/88, 4.5 task/s, elapsed: 15s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>>>       ] 70/88, 4.5 task/s, elapsed: 16s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>>>       ] 71/88, 4.5 task/s, elapsed: 16s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 72/88, 4.5 task/s, elapsed: 16s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 73/88, 4.5 task/s, elapsed: 16s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 74/88, 4.5 task/s, elapsed: 16s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>>>     ] 75/88, 4.5 task/s, elapsed: 17s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>>>     ] 76/88, 4.5 task/s, elapsed: 17s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 77/88, 4.5 task/s, elapsed: 17s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 78/88, 4.5 task/s, elapsed: 17s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 79/88, 4.5 task/s, elapsed: 18s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 80/88, 4.5 task/s, elapsed: 18s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 81/88, 4.5 task/s, elapsed: 18s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 82/88, 4.5 task/s, elapsed: 18s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 83/88, 4.5 task/s, elapsed: 18s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 84/88, 4.5 task/s, elapsed: 19s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 85/88, 4.5 task/s, elapsed: 19s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 86/88, 4.5 task/s, elapsed: 19s, ETA:     0s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 87/88, 4.5 task/s, elapsed: 19s, ETA:     0s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 88/88, 4.4 task/s, elapsed: 20s, ETA:     0sval	 mse:172.3916015625, mae:1648.625
Epoch: 1, Steps: 787 | Lr: 0.0000000 | Train Loss: 0.0026649 | Vali Loss: 0.0014965

Validation loss decreased (inf --> 0.001496).  Saving model ...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> testing  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[                                                  ] 0/63, elapsed: 0s, ETA:[                                  ] 1/63, 1.0 task/s, elapsed: 1s, ETA:    60s[>                                 ] 2/63, 1.7 task/s, elapsed: 1s, ETA:    36s[>                                 ] 3/63, 2.2 task/s, elapsed: 1s, ETA:    27s[>>                                ] 4/63, 2.6 task/s, elapsed: 2s, ETA:    22s[>>                                ] 5/63, 3.0 task/s, elapsed: 2s, ETA:    20s[>>>                               ] 6/63, 3.2 task/s, elapsed: 2s, ETA:    18s[>>>                               ] 7/63, 3.5 task/s, elapsed: 2s, ETA:    16s[>>>>                              ] 8/63, 3.7 task/s, elapsed: 2s, ETA:    15s[>>>>                              ] 9/63, 3.8 task/s, elapsed: 2s, ETA:    14s[>>>>>                            ] 10/63, 4.0 task/s, elapsed: 3s, ETA:    13s[>>>>>                            ] 11/63, 4.1 task/s, elapsed: 3s, ETA:    13s[>>>>>>                           ] 12/63, 4.2 task/s, elapsed: 3s, ETA:    12s[>>>>>>                           ] 13/63, 4.3 task/s, elapsed: 3s, ETA:    12s[>>>>>>>                          ] 14/63, 4.4 task/s, elapsed: 3s, ETA:    11s[>>>>>>>                          ] 15/63, 4.5 task/s, elapsed: 3s, ETA:    11s[>>>>>>>>                         ] 16/63, 4.5 task/s, elapsed: 4s, ETA:    10s[>>>>>>>>                         ] 17/63, 4.6 task/s, elapsed: 4s, ETA:    10s[>>>>>>>>>                        ] 18/63, 4.7 task/s, elapsed: 4s, ETA:    10s[>>>>>>>>>                        ] 19/63, 4.7 task/s, elapsed: 4s, ETA:     9s[>>>>>>>>>>                       ] 20/63, 4.8 task/s, elapsed: 4s, ETA:     9s[>>>>>>>>>>>                      ] 21/63, 4.8 task/s, elapsed: 4s, ETA:     9s[>>>>>>>>>>>                      ] 22/63, 4.9 task/s, elapsed: 5s, ETA:     8s[>>>>>>>>>>>>                     ] 23/63, 4.9 task/s, elapsed: 5s, ETA:     8s[>>>>>>>>>>>>                     ] 24/63, 4.9 task/s, elapsed: 5s, ETA:     8s[>>>>>>>>>>>>>                    ] 25/63, 5.0 task/s, elapsed: 5s, ETA:     8s[>>>>>>>>>>>>>                    ] 26/63, 5.0 task/s, elapsed: 5s, ETA:     7s[>>>>>>>>>>>>>>                   ] 27/63, 5.0 task/s, elapsed: 5s, ETA:     7s[>>>>>>>>>>>>>>                   ] 28/63, 5.1 task/s, elapsed: 6s, ETA:     7s[>>>>>>>>>>>>>>>                  ] 29/63, 5.1 task/s, elapsed: 6s, ETA:     7s[>>>>>>>>>>>>>>>                  ] 30/63, 5.1 task/s, elapsed: 6s, ETA:     6s[>>>>>>>>>>>>>>>>                 ] 31/63, 5.1 task/s, elapsed: 6s, ETA:     6s[>>>>>>>>>>>>>>>>                 ] 32/63, 5.2 task/s, elapsed: 6s, ETA:     6s[>>>>>>>>>>>>>>>>>                ] 33/63, 5.2 task/s, elapsed: 6s, ETA:     6s[>>>>>>>>>>>>>>>>>                ] 34/63, 5.2 task/s, elapsed: 7s, ETA:     6s[>>>>>>>>>>>>>>>>>>               ] 35/63, 5.2 task/s, elapsed: 7s, ETA:     5s[>>>>>>>>>>>>>>>>>>               ] 36/63, 5.2 task/s, elapsed: 7s, ETA:     5s[>>>>>>>>>>>>>>>>>>>              ] 37/63, 5.2 task/s, elapsed: 7s, ETA:     5s[>>>>>>>>>>>>>>>>>>>              ] 38/63, 5.3 task/s, elapsed: 7s, ETA:     5s[>>>>>>>>>>>>>>>>>>>>             ] 39/63, 5.3 task/s, elapsed: 7s, ETA:     5s[>>>>>>>>>>>>>>>>>>>>             ] 40/63, 5.3 task/s, elapsed: 8s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>            ] 41/63, 5.3 task/s, elapsed: 8s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>           ] 42/63, 5.3 task/s, elapsed: 8s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>           ] 43/63, 5.3 task/s, elapsed: 8s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>          ] 44/63, 5.4 task/s, elapsed: 8s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>          ] 45/63, 5.4 task/s, elapsed: 8s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>         ] 46/63, 5.4 task/s, elapsed: 9s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>         ] 47/63, 5.4 task/s, elapsed: 9s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>        ] 48/63, 5.4 task/s, elapsed: 9s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>        ] 49/63, 5.4 task/s, elapsed: 9s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>>       ] 50/63, 5.4 task/s, elapsed: 9s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>       ] 51/63, 5.4 task/s, elapsed: 9s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 52/63, 5.4 task/s, elapsed: 10s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>      ] 53/63, 5.5 task/s, elapsed: 10s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>     ] 54/63, 5.5 task/s, elapsed: 10s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>     ] 55/63, 5.5 task/s, elapsed: 10s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 56/63, 5.5 task/s, elapsed: 10s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 57/63, 5.5 task/s, elapsed: 10s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 58/63, 5.5 task/s, elapsed: 11s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 59/63, 5.5 task/s, elapsed: 11s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 60/63, 5.5 task/s, elapsed: 11s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 61/63, 5.5 task/s, elapsed: 11s, ETA:     0s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 62/63, 5.5 task/s, elapsed: 11s, ETA:     0s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 63/63, 5.6 task/s, elapsed: 11s, ETA:     0s