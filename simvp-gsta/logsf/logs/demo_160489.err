/ext3/miniconda3/lib/python3.11/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
usage: train.py [-h] [--device DEVICE] [--dist] [--display_step DISPLAY_STEP]
                [--res_dir RES_DIR] [--ex_name EX_NAME] [--use_gpu USE_GPU]
                [--fp16] [--torchscript] [--seed SEED] [--diff_seed] [--fps]
                [--empty_cache] [--find_unused_parameters]
                [--broadcast_buffers] [--resume_from RESUME_FROM]
                [--auto_resume] [--test] [--inference] [--deterministic]
                [--launcher {none,pytorch,slurm,mpi}]
                [--local_rank LOCAL_RANK] [--port PORT]
                [--batch_size BATCH_SIZE] [--val_batch_size VAL_BATCH_SIZE]
                [--num_workers NUM_WORKERS] [--data_root DATA_ROOT]
                [--dataname DATANAME] [--pre_seq_length PRE_SEQ_LENGTH]
                [--aft_seq_length AFT_SEQ_LENGTH]
                [--total_length TOTAL_LENGTH] [--use_augment]
                [--use_prefetcher] [--drop_last]
                [--method {ConvLSTM,convlstm,CrevNet,crevnet,DMVFN,dmvfn,E3DLSTM,e3dlstm,MAU,mau,MIM,mim,PhyDNet,phydnet,PredNet,prednet,PredRNN,predrnn,PredRNNpp,predrnnpp,PredRNNv2,predrnnv2,SimVP,simvp,TAU,tau}]
                [--config_file CONFIG_FILE] [--model_type MODEL_TYPE]
                [--drop DROP] [--drop_path DROP_PATH] [--overwrite]
                [--epoch EPOCH] [--log_step LOG_STEP] [--opt OPTIMIZER]
                [--opt_eps EPSILON] [--opt_betas BETA [BETA ...]]
                [--momentum M] [--weight_decay WEIGHT_DECAY]
                [--clip_grad NORM] [--clip_mode CLIP_MODE]
                [--early_stop_epoch EARLY_STOP_EPOCH]
                [--no_display_method_info] [--sched SCHEDULER] [--lr LR]
                [--lr_k_decay LR_K_DECAY] [--warmup_lr LR] [--min_lr LR]
                [--final_div_factor FINAL_DIV_FACTOR] [--warmup_epoch N]
                [--decay_epoch N] [--decay_rate RATE]
                [--filter_bias_and_bn FILTER_BIAS_AND_BN]
train.py: error: unrecognized arguments: --local-rank=0
usage: train.py [-h] [--device DEVICE] [--dist] [--display_step DISPLAY_STEP]
                [--res_dir RES_DIR] [--ex_name EX_NAME] [--use_gpu USE_GPU]
                [--fp16] [--torchscript] [--seed SEED] [--diff_seed] [--fps]
                [--empty_cache] [--find_unused_parameters]
                [--broadcast_buffers] [--resume_from RESUME_FROM]
                [--auto_resume] [--test] [--inference] [--deterministic]
                [--launcher {none,pytorch,slurm,mpi}]
                [--local_rank LOCAL_RANK] [--port PORT]
                [--batch_size BATCH_SIZE] [--val_batch_size VAL_BATCH_SIZE]
                [--num_workers NUM_WORKERS] [--data_root DATA_ROOT]
                [--dataname DATANAME] [--pre_seq_length PRE_SEQ_LENGTH]
                [--aft_seq_length AFT_SEQ_LENGTH]
                [--total_length TOTAL_LENGTH] [--use_augment]
                [--use_prefetcher] [--drop_last]
                [--method {ConvLSTM,convlstm,CrevNet,crevnet,DMVFN,dmvfn,E3DLSTM,e3dlstm,MAU,mau,MIM,mim,PhyDNet,phydnet,PredNet,prednet,PredRNN,predrnn,PredRNNpp,predrnnpp,PredRNNv2,predrnnv2,SimVP,simvp,TAU,tau}]
                [--config_file CONFIG_FILE] [--model_type MODEL_TYPE]
                [--drop DROP] [--drop_path DROP_PATH] [--overwrite]
                [--epoch EPOCH] [--log_step LOG_STEP] [--opt OPTIMIZER]
                [--opt_eps EPSILON] [--opt_betas BETA [BETA ...]]
                [--momentum M] [--weight_decay WEIGHT_DECAY]
                [--clip_grad NORM] [--clip_mode CLIP_MODE]
                [--early_stop_epoch EARLY_STOP_EPOCH]
                [--no_display_method_info] [--sched SCHEDULER] [--lr LR]
                [--lr_k_decay LR_K_DECAY] [--warmup_lr LR] [--min_lr LR]
                [--final_div_factor FINAL_DIV_FACTOR] [--warmup_epoch N]
                [--decay_epoch N] [--decay_rate RATE]
                [--filter_bias_and_bn FILTER_BIAS_AND_BN]
train.py: error: unrecognized arguments: --local-rank=1
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 13739) of binary: /ext3/miniconda3/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/ext3/miniconda3/lib/python3.11/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/ext3/miniconda3/lib/python3.11/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/ext3/miniconda3/lib/python3.11/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/ext3/miniconda3/lib/python3.11/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/ext3/miniconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tools/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-12-06_02:16:42
  host      : b-6-8.c.hpc-slurm-9c75.internal
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 13740)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-12-06_02:16:42
  host      : b-6-8.c.hpc-slurm-9c75.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 13739)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
